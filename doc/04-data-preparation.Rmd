---
title: "Data Preparation "
subtitle: "res431"
author: "pbrunier@cogne.com"
version: 1.0 
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    css: ./custom.css
    df_print: paged
    gallery: no
    highlight: default
    html_document: null
    lightbox: yes
    number_sections: yes
    self_contained: yes
    thumbnails: no
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      error = FALSE,
                      fig.height = 7,
                      fig.width = 10,
                      collapse = TRUE,
                      cols.print=20)
rm(list = ls(all.names = TRUE))

require(dplyr)
require(kableExtra)
require(readr)
require(reticulate)
source('~/dev/res431/R/function.R')
```

# Data Building 

Il database finale per l'analisi dei dati sarà il risultato di un join delle tabelle mappate in [03-data-mapping](./03-data-mapping.html)

Il dato è riassunto per odp. Pertanto, le triple di kv verranno riassunte nei seguenti termini:

+ valore medio
+ deviazione standard
+ numerosità : questa colonna in realtà è costante (pari a 3) per tutti gli odp.

Comincio con il caricate tutte le tabelle singole

```{python load_all_data}
import pandas as pd
import os 
import numpy as np

def MapTable(datafile,header_file):
    header = pd.read_csv(header_file,sep=';')
    
    try:
      df = pd.read_csv(datafile, sep=';',encoding = 'utf-8',
                     # dtype = header.data_type.values,
                     )
    except UnicodeDecodeError:
      df = pd.read_csv(datafile, sep=';',encoding = 'cp1252',
                     # dtype = header.data_type.values,
                     )
      
    
    df = df.set_axis(header.col_name, axis=1, inplace=False)
    
    #drop delle colonne non interessanti
    to_use = header[header.grab != 0].col_name
    return df[to_use]


data_path = '/data/res431/'
head_path = os.path.join(data_path,'header')

file_tvb = os.path.join(data_path,'dati_KV_TVB.txt')
file_ch = os.path.join(data_path,'dati_CH.txt')
# file_rm = os.path.join(data_path,'dati_RM.txt')
# file_CC = 'dati_CCO.txt'
file_rinv1 = os.path.join(data_path,'dati_RINV1.txt')
file_rinv2 = os.path.join(data_path,'dati_RINV2.txt')


head_ch = os.path.join(head_path,'header_CH.csv')
head_tvb = os.path.join(head_path,'header_KVTVB.csv')
# head_rm = os.path.join(head_path,'header_rm.csv')
# head_cc = 'header_CCO.csv'
head_r1 = os.path.join(head_path,'header_RINV1.csv')
head_r2 = os.path.join(head_path,'header_RINV2.csv')


df_ch = MapTable(file_ch,head_ch)
df_tvb = MapTable(file_tvb,head_tvb)
df_rinv1 = MapTable(file_rinv1,head_r1)
df_rinv2 = MapTable(file_rinv2,head_r2)
# df_cc = MapTable(file_CC,head_cc)
# df_rm = MapTable(file_rm,head_rm)


```

Facciamo in seguito un join dei dati. 

Le chimiche sulla colata
Il resto sull'odp

La base dati iniziale è quella del TVB, in quanto contiene già le triple di Kv ripetute per l'odp di riferimento. Così facendo, ho la possibilità di aggangiare i dati sull'odp in modo da 'ripetere' i dati.

```{python join_all_data}
df = df_tvb.copy()
df_TT = pd.merge(df_rinv1,df_rinv2,on ='odp')

print('TT contiene i dati relativi a %d odp' %(len(np.unique(df_TT.odp)))
print('LAM contiene i dati relativi a %d odp' %(len(np.unique(df_tvb.odp))))
print('ACC contiene i dati relativi a %d colate' %(len(np.unique(df_ch.colata)))

# print('colate TVB')
# colate_tvb = np.unique(df_tvb.colata)
# for cc in colate_tvb:
#   print('\t- %s' %cc)
#   
# print('colate ACC')
# colate_ch = np.unique(df_ch.colata)
# for cc in colate_ch:
#   print('\t- %s' %cc)

df = pd.merge(df,df_ch,on='colata')
# df = pd.merge(df,df_cc,on='colata')
df = pd.merge(df,df_TT,on='odp')

#setto alcune cose
# res431 <- res431 %>%
#   mutate(data_ins=as.Date(data_ins, format = "%d/%m/%Y"))



# ordino le features

ordered_columns = ['data_ins', 'anno','marca', 'odp','sk','colata', 'colata_aod',
       'kv',  'T_media', 'min_1050', 'min_1100', 'T_G14', 'C',
       'S', 'P', 'Si', 'Mn', 'Cr', 'Ni', 'Mo', 'Cu', 'Sn', 'Al', 'V', 'Co',
       'Ti', 'Nb', 'Ca', 'N2',
       'peso_rinv1', 'peso_batch_rinv2',
        'batch_rinv1','batch_rinv2','forno_rinv1','forno_rinv2']
# ordered_columns = df.columns
df = df[ordered_columns]

```
<!-- ordino le colonne -->

<!-- ```{python grouped_by_triple} -->

<!-- dfg = df.groupby('odp').mean() -->
<!-- dfg['kv_std'] = df.groupby('odp').std().kv_valori -->
<!-- dfg['kv_N'] = df.groupby('odp').count().colata -->
<!-- dfg = dfg.reset_index() -->

<!-- ordered_cols = ['kv_valori', -->
<!--        'temp_media','temp_sup', 'temp_inf', 'min_over_1050', 'min_over_1100', -->
<!--        'C', 'S', 'P','Si', 'Mn', 'Cr', 'Ni', 'Mo', 'Cu', 'Sn', 'Al', 'V', -->
<!--        'Co', 'Ti', 'Nb','Ca', 'N2', -->
<!--        'PA', 'Cr_eq', 'Ni_eq', 'Cr_eq_over_Ni_eq', -->
<!--        'peso_rinv1','peso_batch_rinv2','T_pirG14'] -->

<!-- # salvo il file -->
<!-- dfg = dfg[ordered_cols] -->



<!-- ``` -->
<!-- raggruppo per colata -->
<!-- ```{python df_colate} -->

<!-- dfc = df.groupby('colata').mean() -->

<!-- dfc['kv_stddev'] = df.groupby('colata').std().kv_valori -->
<!-- dfc['kv_ncount'] = df.groupby('colata').count().kv_valori -->


<!-- ``` -->

salvo come .pkl 

```{python save_pickle}
# dfg.to_pickle('/data/res431/res431_grouped.pkl')
df.to_pickle('/data/res431/res431.pkl')
# dfc.to_pickle('/data/res431/res431_colate.pkl')

```

salvo come .rds

```{r save_rds}
# dati_res431 <- py$dfg
# write_rds(dati_res431,'/data/res431/res431_grouped.rds')

res431 <- py$df
write_rds(res431,'/data/res431/res431.rds')

# res_colate <- py$dfc
# write_rds(res_colate,'/data/res431/res431_colate.rds')

```
